version: '3.9'
services:
  kafdrop:
      image: obsidiandynamics/kafdrop
      restart: always
      ports:
        - "9000:9000"
      environment:
        KAFKA_BROKERCONNECT: "broker0:29092,broker1:29093,broker2:29094"
      depends_on:
        - broker0
        - broker1
        - broker2
        - schema-registry

  kafka-ui: # https://github.com/provectus/kafka-ui/blob/master/documentation/compose/kafka-ui-arm64.yaml
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    ports:
      - "8085:8085"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: broker0:29092
      KAFKA_CLUSTERS_0_METRICS_PORT: 9997
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: first
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://kafka-connect:8083
      DYNAMIC_CONFIG_ENABLED: 'true'  # not necessary, added for tests
      KAFKA_CLUSTERS_0_AUDIT_TOPICAUDITENABLED: 'true'
      KAFKA_CLUSTERS_0_AUDIT_CONSOLEAUDITENABLED: 'true'
      KAFKA_CLUSTERS_1_NAME: second_local
      KAFKA_CLUSTERS_1_BOOTSTRAPSERVERS: broker1:29093
      KAFKA_CLUSTERS_1_METRICS_PORT: 9998
      KAFKA_CLUSTERS_1_SCHEMAREGISTRY: http://schema-registry:8081
      KAFKA_CLUSTERS_2_NAME: third_local
      KAFKA_CLUSTERS_2_BOOTSTRAPSERVERS: broker2:29094
      KAFKA_CLUSTERS_2_METRICS_PORT: 9999
      KAFKA_CLUSTERS_2_SCHEMAREGISTRY: http://schema-registry:8081
    depends_on:
      - broker0
      - broker1
      - broker2
      - kafka-connect
      - schema-registry

  zk:
    image: confluentinc/cp-zookeeper:7.6.0
    hostname: zk
    container_name: zk
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker0:
    image: confluentinc/cp-kafka:7.6.0
    hostname: broker0
    container_name: broker0
    restart: always
    depends_on:
      - zk
    ports:
      - "29092:29092"
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 0
      KAFKA_ZOOKEEPER_CONNECT: 'zk:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker0:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_MIN_INSYNC_REPLICAS: 2 # min 2 in-sync replicas for each partition.
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3 # Each partition will be replicated across 3 brokers.
      KAFKA_NUM_PARTITIONS: 3  # 3 partitions for new topics, impacting parallelism and throughput.

  broker1:
    image: confluentinc/cp-kafka:7.6.0
    hostname: broker1
    container_name: broker1
    restart: always
    depends_on:
      - zk
    ports:
      - "29093:29093"
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zk:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker1:29093,PLAINTEXT_HOST://localhost:9093
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_NUM_PARTITIONS: 3

  broker2:
    image: confluentinc/cp-kafka:7.6.0
    hostname: broker2
    container_name: broker2
    restart: always
    depends_on:
      - zk
    ports:
      - "29094:29094"
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: 'zk:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker2:29094,PLAINTEXT_HOST://localhost:9094
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_NUM_PARTITIONS: 3

  schema-registry: # https://docs.confluent.io/platform/current/installation/docker/config-reference.html#sr-long-configuration
    image: confluentinc/cp-schema-registry:7.6.0
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - broker0
      - broker1
      - broker2
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker0:29092,broker1:29093,broker2:29094'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_SCHEMA_COMPABILITY_LEVEL: FULL

  kafka-connect:
    build:
      context: ./datagen
      dockerfile: Dockerfile
    container_name: kafka-connect
    hostname: kafka-connect
    depends_on:
      - broker0
      - broker1
      - broker2
      - schema-registry
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "broker0:29092,broker1:29093,broker2:29094"
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: kafka-connect
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components,/data/connect-jars

  cli-tools: # cli to execute commands on all broker
    image: confluentinc/cp-kafka:7.6.0
    container_name: cli-tools
    entrypoint: "sleep infinity"
    restart: always

#  kafka-init-topics:
#    image: confluentinc/cp-kafka:7.6.0
#    volumes:
#      - ./data/message.json:/data/message.json
#    depends_on:
#      - kafka1
#    command: "bash -c 'echo Waiting for Kafka to be ready... && \
#                 cub kafka-ready -b kafka1:29092 1 30 && \
#                 kafka-topics --create --topic second.users --partitions 3 --replication-factor 1 --if-not-exists --bootstrap-server kafka1:29092 && \
#                 kafka-topics --create --topic second.messages --partitions 2 --replication-factor 1 --if-not-exists --bootstrap-server kafka1:29092 && \
#                 kafka-topics --create --topic first.messages --partitions 2 --replication-factor 1 --if-not-exists --bootstrap-server kafka0:29092 && \
#                 kafka-console-producer --bootstrap-server kafka1:29092 -topic second.users < /data/message.json'"